{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seat belt, seatbelt\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EfficientNetImageProcessor, EfficientNetForImageClassification\n",
    "from PIL import Image\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from distracted.dataset_loader import dataset_loader\n",
    "\n",
    "image = Image.open('data/imgs/train/c0/img_34.jpg')\n",
    "\n",
    "preprocessor = EfficientNetImageProcessor.from_pretrained(\"google/efficientnet-b7\")\n",
    "model = EfficientNetForImageClassification.from_pretrained(\"google/efficientnet-b7\")\n",
    "\n",
    "inputs = preprocessor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label]),\n",
    "# device = torch.device(\"cuda\")\n",
    "device = torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model(**inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_loader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to guide for adding additional layers to pretrained model](https://medium.com/analytics-vidhya/how-to-add-additional-layers-in-a-pre-trained-model-using-pytorch-5627002c75a5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet_Adapter_Finetuning(EfficientNetForImageClassification):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(self).__init__()\n",
    "        self.model = EfficientNetForImageClassification.from_pretrained(\"google/efficientnet-b7\")\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor = None,): # Add input stuff\n",
    "        # Get output for EfficientNetModel then do EfficientNetForImageClassification part of forward()\n",
    "        # Should have self.embeddings from EfficientNetModel which will probably not need to be retrained\n",
    "        embedding_output = self.embeddings(pixel_values)\n",
    "\n",
    "        # self.encoder is likely what needs to be finetuned\n",
    "        # Encoder is EfficentNetEncoder\n",
    "        # Forward pass of Encoder loops through blocks and updates hidden_states\n",
    "        # Endcoder with potential edits looks like following:\n",
    "\n",
    "        # for block,adapter in (self.blocks,self.adapters):\n",
    "        #     hidden_states = block(hidden_states)\n",
    "        #     hidden_states = adapter(hidden_states)\n",
    "\n",
    "        # hidden_states = self.top_conv(hidden_states)\n",
    "        # hidden_states = self.top_bn(hidden_states)\n",
    "        # hidden_states = self.top_activation(hidden_states)\n",
    "        \n",
    "        # Idea is to add adapter layer after each loop \n",
    "\n",
    "        # Maybe Make EfficinetNet_Adapter_Encoder model then change self.model.something.encoder to this new one\n",
    "        # Can initialize it with pretrained so self.blocks is the same\n",
    "\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            output_hidden_states=self.config.output_hidden_states,\n",
    "            )\n",
    "\n",
    "        # EfficientNetForImageClassification forward takes output of EfficientNetModel and does the following:\n",
    "        # outputs = self.efficientnet(pixel_values, output_hidden_states=output_hidden_states, return_dict=return_dict)\n",
    "\n",
    "        # pooled_output = outputs.pooler_output if return_dict else outputs[1]\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "        # logits = self.classifier(pooled_output)\n",
    "        # logits = self.classifier_act(logits)\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_instance = model.efficientnet.encoder\n",
    "encoder_class = encoder_instance.__class__\n",
    "# config = model.config\n",
    "\n",
    "# block_input = torch.randn(1,64,300,300)\n",
    "# encoder_block_output_class = encoder_instance.blocks[0](block_input).__class__\n",
    "# print(encoder_block_output_class)\n",
    "\n",
    "# Following two functions taken from modeling_efficientnet.py\n",
    "def round_repeats(repeats,depth_coefficient):\n",
    "            # Round number of block repeats based on depth multiplier.\n",
    "            return int(math.ceil(depth_coefficient * repeats))\n",
    "def round_filters(config, num_channels: int):\n",
    "    r\"\"\"\n",
    "    Round number of filters based on depth multiplier.\n",
    "    \"\"\"\n",
    "    divisor = config.depth_divisor\n",
    "    num_channels *= config.width_coefficient\n",
    "    new_dim = max(divisor, int(num_channels + divisor / 2) // divisor * divisor)\n",
    "\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_dim < 0.9 * num_channels:\n",
    "        new_dim += divisor\n",
    "\n",
    "    return int(new_dim)\n",
    "\n",
    "\n",
    "\n",
    "class EfficientNetAdapterEncoding(encoder_class):\n",
    "    def __init__(self, model):\n",
    "        encoder_instance = model.efficientnet.encoder\n",
    "        config = model.config\n",
    "        super().__init__(config)\n",
    "    \n",
    "        self.blocks = encoder_instance.blocks\n",
    "        self.top_conv = encoder_instance.top_conv\n",
    "        self.top_bn = encoder_instance.top_bn\n",
    "        self.top_activation = encoder_instance.top_activation\n",
    "        self.adapters = []\n",
    "\n",
    "        \n",
    "\n",
    "        num_base_blocks = len(config.in_channels)\n",
    "        adapter_dimensions = []\n",
    "        block_dimensions = []\n",
    "        for i in range(num_base_blocks):\n",
    "            block_out_dim = round_filters(config,config.out_channels[i])\n",
    "            block_in_dim = round_filters(config,config.in_channels[i]) \n",
    "            for _ in range(round_repeats(config.num_block_repeats[i],config.depth_coefficient)):\n",
    "                block_dimensions.append((block_in_dim,block_out_dim))\n",
    "        for j in range(len(block_dimensions)-1):\n",
    "             adapter_dimension_input = block_dimensions[j][1] # output of previous block\n",
    "             adapter_dimension_output = block_dimensions[j+1][0] # input of next block\n",
    "             adapter_dimensions.append((adapter_dimension_input,adapter_dimension_output))\n",
    "        last_adapter_dimension = block_dimensions[-1][1]\n",
    "        adapter_dimensions.append((last_adapter_dimension,last_adapter_dimension))\n",
    "\n",
    "        for adapter_dimension in adapter_dimensions:\n",
    "             self.adapters.append(nn.Linear(*adapter_dimension))\n",
    "        # Wrong dimensions\n",
    "        # Also wayyyyyyyyyyyyyyyy too many parameters if right dimensions.....\n",
    "        # Don't use Linear\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states,\n",
    "                output_hidden_states = False,\n",
    "                return_dict = True):\n",
    "        \n",
    "        for block,adapter in zip(self.blocks,self.adapters):\n",
    "            hidden_states = block(hidden_states)\n",
    "            hidden_states = adapter(hidden_states)\n",
    "        \n",
    "        hidden_states = self.top_conv(hidden_states)\n",
    "        hidden_states = self.top_bn(hidden_states)\n",
    "        hidden_states = self.top_activation(hidden_states)\n",
    "            \n",
    "        return (hidden_states,None) # Should work as forward pass just takes encoder_output[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_encoding = EfficientNetAdapterEncoding(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_instance.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 300, 300])\n",
      "Linear(in_features=32, out_features=64, bias=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (9600x300 and 32x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(encoder_block_output\u001b[39m.\u001b[39msize())\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(adapter_encoding\u001b[39m.\u001b[39madapters[\u001b[39m0\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m adapter_encoding\u001b[39m.\u001b[39;49madapters[\u001b[39m0\u001b[39;49m](encoder_block_output)\n",
      "File \u001b[1;32mc:\\Users\\Johan\\Dropbox\\Uni\\ITU\\Year_4\\AdvancedMachineLearning\\Distracted-Drivers\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Johan\\Dropbox\\Uni\\ITU\\Year_4\\AdvancedMachineLearning\\Distracted-Drivers\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (9600x300 and 32x64)"
     ]
    }
   ],
   "source": [
    "block_input = torch.randn(1,64,300,300)\n",
    "encoder_block_output = encoder_instance.blocks[0](block_input)\n",
    "print(encoder_block_output.size())\n",
    "print(adapter_encoding.adapters[0])\n",
    "adapter_encoding.adapters[0](encoder_block_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 8.00 GiB total capacity; 6.93 GiB already allocated; 0 bytes free; 7.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m block \u001b[39m=\u001b[39m encoder_instance\u001b[39m.\u001b[39mblocks[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      8\u001b[0m block\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m summary(block,input_size\u001b[39m=\u001b[39;49m(\u001b[39m640\u001b[39;49m,\u001b[39m300\u001b[39;49m,\u001b[39m300\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Johan\\Dropbox\\Uni\\ITU\\Year_4\\AdvancedMachineLearning\\Distracted-Drivers\\venv\\lib\\site-packages\\torchsummary\\torchsummary.py:60\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     57\u001b[0m     input_size \u001b[39m=\u001b[39m [input_size]\n\u001b[0;32m     59\u001b[0m \u001b[39m# batch_size of 2 for batchnorm\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m x \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39min_size)\u001b[39m.\u001b[39mtype(dtype) \u001b[39mfor\u001b[39;00m in_size \u001b[39min\u001b[39;00m input_size]\n\u001b[0;32m     61\u001b[0m \u001b[39m# print(type(x[0]))\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[39m# create properties\u001b[39;00m\n\u001b[0;32m     64\u001b[0m summary \u001b[39m=\u001b[39m OrderedDict()\n",
      "File \u001b[1;32mc:\\Users\\Johan\\Dropbox\\Uni\\ITU\\Year_4\\AdvancedMachineLearning\\Distracted-Drivers\\venv\\lib\\site-packages\\torchsummary\\torchsummary.py:60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m     input_size \u001b[39m=\u001b[39m [input_size]\n\u001b[0;32m     59\u001b[0m \u001b[39m# batch_size of 2 for batchnorm\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m x \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mrand(\u001b[39m2\u001b[39;49m, \u001b[39m*\u001b[39;49min_size)\u001b[39m.\u001b[39;49mtype(dtype) \u001b[39mfor\u001b[39;00m in_size \u001b[39min\u001b[39;00m input_size]\n\u001b[0;32m     61\u001b[0m \u001b[39m# print(type(x[0]))\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[39m# create properties\u001b[39;00m\n\u001b[0;32m     64\u001b[0m summary \u001b[39m=\u001b[39m OrderedDict()\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 8.00 GiB total capacity; 6.93 GiB already allocated; 0 bytes free; 7.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "encoder_instance = model.efficientnet.encoder\n",
    "encoder_instance.to(device)\n",
    "# adapter_encoding.to(\"cpu\")\n",
    "from torchsummary import summary\n",
    "# use this later for checking trainable params etc\n",
    "# block 0 has 64, 300, 300\n",
    "block = encoder_instance.blocks[-1]\n",
    "block.to(device)\n",
    "summary(block,input_size=(640,300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 300, 300])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.efficientnet.embeddings(**inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through named_parameters and set adapters to True and everything else to False?\n",
    "# for name,para in model.named_parameters():\n",
    "#     print(name, para.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_dim 64 out_dim 32\n",
      "in_dim 64 out_dim 32\n",
      "in_dim 64 out_dim 32\n",
      "in_dim 64 out_dim 32\n",
      "in_dim 32 out_dim 48\n",
      "in_dim 32 out_dim 48\n",
      "in_dim 32 out_dim 48\n",
      "in_dim 32 out_dim 48\n",
      "in_dim 32 out_dim 48\n",
      "in_dim 32 out_dim 48\n",
      "in_dim 32 out_dim 48\n",
      "in_dim 48 out_dim 80\n",
      "in_dim 48 out_dim 80\n",
      "in_dim 48 out_dim 80\n",
      "in_dim 48 out_dim 80\n",
      "in_dim 48 out_dim 80\n",
      "in_dim 48 out_dim 80\n",
      "in_dim 48 out_dim 80\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 80 out_dim 160\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 160 out_dim 224\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 224 out_dim 384\n",
      "in_dim 384 out_dim 640\n",
      "in_dim 384 out_dim 640\n",
      "in_dim 384 out_dim 640\n",
      "in_dim 384 out_dim 640\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
